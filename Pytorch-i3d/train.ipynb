{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9283810-1cc3-400b-ab0d-2b29ad77917f",
   "metadata": {},
   "source": [
    "### Finetuning and training the Pytorch-i3d model\n",
    "Code taken from: https://github.com/piergiaj/pytorch-i3d/blob/master/train_i3d.py \n",
    "\n",
    "Note: This code was written for PyTorch 0.3. Version 0.4 and newer may cause issues."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fbf0b21-10bf-490d-9dd2-95e35d3114e6",
   "metadata": {},
   "source": [
    "To-dos:\n",
    "1. extract videos and add labels (the line `return images, 0` assigns the label 0 to every images in the code for class `Dataset`. I have chosen the top few single-class labels in `preprocess.ipynb`, see the last cell to get the labels and associated videos through the video names, `v_names`)\n",
    "2. streamline process for training pre-trained model (loaded via `i3d.load_state_dict(torch.load('rgb_imagenet.pt'))`) (no need to change layers yet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56422195-b081-4f3d-8b9a-8d541d0bad5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]='2'\n",
    "import sys\n",
    "import argparse\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "import videotransforms\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from pytorch_i3d import InceptionI3d\n",
    "\n",
    "import numpy as np\n",
    "import glob\n",
    "import random\n",
    "\n",
    "from tensorboardX import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a76d20a-aa79-438d-95f9-b62477c305b3",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "num_samples should be a positive integer value, but got num_samples=0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 34\u001b[0m\n\u001b[1;32m     32\u001b[0m random\u001b[38;5;241m.\u001b[39mshuffle(paths)\n\u001b[1;32m     33\u001b[0m d\u001b[38;5;241m=\u001b[39mdataset(paths\u001b[38;5;241m=\u001b[39mpaths[:\u001b[38;5;241m800\u001b[39m])\n\u001b[0;32m---> 34\u001b[0m loader \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataLoader\u001b[49m\u001b[43m(\u001b[49m\u001b[43md\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdrop_last\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/cos429/lib/python3.8/site-packages/torch/utils/data/dataloader.py:351\u001b[0m, in \u001b[0;36mDataLoader.__init__\u001b[0;34m(self, dataset, batch_size, shuffle, sampler, batch_sampler, num_workers, collate_fn, pin_memory, drop_last, timeout, worker_init_fn, multiprocessing_context, generator, prefetch_factor, persistent_workers, pin_memory_device)\u001b[0m\n\u001b[1;32m    349\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# map-style\u001b[39;00m\n\u001b[1;32m    350\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m shuffle:\n\u001b[0;32m--> 351\u001b[0m         sampler \u001b[38;5;241m=\u001b[39m \u001b[43mRandomSampler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgenerator\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    352\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    353\u001b[0m         sampler \u001b[38;5;241m=\u001b[39m SequentialSampler(dataset)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/cos429/lib/python3.8/site-packages/torch/utils/data/sampler.py:107\u001b[0m, in \u001b[0;36mRandomSampler.__init__\u001b[0;34m(self, data_source, replacement, num_samples, generator)\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreplacement should be a boolean value, but got \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    104\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreplacement=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplacement))\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_samples, \u001b[38;5;28mint\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_samples \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 107\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_samples should be a positive integer \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    108\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue, but got num_samples=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_samples))\n",
      "\u001b[0;31mValueError\u001b[0m: num_samples should be a positive integer value, but got num_samples=0"
     ]
    }
   ],
   "source": [
    "class dataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, paths, num_samples=16): # num_samples cannot be lower than 16\n",
    "        self.num_samples = num_samples\n",
    "        self.frames = dict()\n",
    "        for p in paths:\n",
    "            self.frames[p] = sorted(glob.glob(p+\"/*.jpg\"))\n",
    "        self.data = paths\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        p = self.data[idx]\n",
    "        num_frames = len(self.frames[p])-1\n",
    "        sampled_idx = np.linspace(0, num_frames, self.num_samples)\n",
    "        images = []\n",
    "        for i in sampled_idx:\n",
    "            image = torchvision.io.read_image(self.frames[p][int(i)])\n",
    "            small_dim = min(image.shape[-2:])\n",
    "            image = torchvision.transforms.functional.center_crop(image, (small_dim, small_dim))\n",
    "            image = torchvision.transforms.functional.resize(image, (224, 224), antialias=True)\n",
    "            images.append(image)\n",
    "        images = torch.stack(images, axis=1)\n",
    "        images = (images/255)*2 - 1 # values are between -1 and 1\n",
    "        return images, 0 # 0 is just a placeholder for labels, getitem return data, labels \n",
    "        \n",
    "video_frames_path = \"/scratch/network/hishimwe/image\" \n",
    "# add code here to only extract the videos with v_names and v_labels from preprocess.ipynb \n",
    "paths = glob.glob(video_frames_path+\"/*\")\n",
    "random.seed(0)\n",
    "random.shuffle(paths)\n",
    "d=dataset(paths=paths[:800]) # first 800, change as needed\n",
    "loader = torch.utils.data.DataLoader(d, shuffle=True, batch_size=10, drop_last=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b34bc2b-1dc1-4b2d-b31b-ea80b8528827",
   "metadata": {},
   "outputs": [],
   "source": [
    "d.__getitem__(0)[0].shape, d.__getitem__(0)[1] # data, label "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "851c4b99-91a9-4787-9fd2-5ba5ac9a0751",
   "metadata": {},
   "source": [
    "Need to make sure that each input has the same number of frames -> either by taking the minimum or even better just do uniform sampling (same number of frames regardless of video length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b8f33d-88ce-487f-be31-ac12c2487082",
   "metadata": {},
   "outputs": [],
   "source": [
    "i3d = InceptionI3d(400, in_channels=3) \n",
    "i3d.load_state_dict(torch.load('rgb_imagenet.pt'))\n",
    "i3d.replace_logits(157) # replace 157 with num_classes\n",
    "#i3d.load_state_dict(torch.load('/ssd/models/000920.pt'))\n",
    "i3d.cuda()\n",
    "i3d(torch.zeros(4,3,16,224,224).cuda()).shape # example input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff9bb17-34d0-4a24-81c1-e9416b9f786e",
   "metadata": {},
   "source": [
    "Example training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dac52e08-f69e-47a4-ac7b-bedf0c8d70ed",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0: loss = 5.229801177978516\n",
      "epoch 0: loss = 2.0972135066986084\n",
      "epoch 0: loss = 0.07437565922737122\n",
      "epoch 0: loss = 0.00036732948501594365\n",
      "epoch 0: loss = 5.3908115660306066e-05\n",
      "epoch 0: loss = 3.576278118089249e-08\n",
      "epoch 0: loss = 0.0\n",
      "epoch 0: loss = 0.0\n",
      "epoch 0: loss = 0.0\n",
      "epoch 0: loss = 0.0\n",
      "epoch 0: loss = 0.0\n",
      "epoch 0: loss = 0.0\n",
      "epoch 0: loss = 0.0\n",
      "epoch 0: loss = 0.0\n",
      "epoch 0: loss = 0.0\n",
      "epoch 0: loss = 0.0\n",
      "epoch 0: loss = 0.0\n",
      "epoch 0: loss = 0.0\n",
      "epoch 0: loss = 0.0\n",
      "epoch 0: loss = 0.0\n",
      "epoch 0: loss = 0.0\n",
      "epoch 0: loss = 0.0\n",
      "epoch 0: loss = 0.0\n",
      "epoch 0: loss = 0.0\n",
      "epoch 0: loss = 0.0\n",
      "epoch 0: loss = 0.0\n",
      "epoch 0: loss = 0.0\n",
      "epoch 0: loss = 0.0\n",
      "epoch 0: loss = 0.0\n",
      "epoch 0: loss = 0.0\n",
      "epoch 0: loss = 0.0\n",
      "epoch 0: loss = 0.0\n",
      "epoch 0: loss = 0.0\n",
      "epoch 0: loss = 0.0\n",
      "epoch 0: loss = 0.0\n",
      "epoch 0: loss = 0.0\n",
      "epoch 0: loss = 0.0\n",
      "epoch 0: loss = 0.0\n",
      "epoch 0: loss = 0.0\n",
      "epoch 0: loss = 0.0\n",
      "epoch 0: loss = 0.0\n",
      "epoch 0: loss = 0.0\n",
      "epoch 0: loss = 0.0\n",
      "epoch 0: loss = 0.0\n",
      "epoch 0: loss = 0.0\n",
      "epoch 0: loss = 0.0\n",
      "epoch 0: loss = 0.0\n",
      "epoch 0: loss = 0.0\n",
      "epoch 0: loss = 0.0\n",
      "epoch 0: loss = 0.0\n",
      "epoch 0: loss = 0.0\n",
      "epoch 0: loss = 0.0\n",
      "epoch 0: loss = 0.0\n",
      "epoch 0: loss = 0.0\n",
      "epoch 0: loss = 0.0\n",
      "epoch 0: loss = 0.0\n",
      "epoch 0: loss = 0.0\n",
      "epoch 0: loss = 0.0\n",
      "epoch 0: loss = 0.0\n",
      "epoch 0: loss = 0.0\n",
      "epoch 0: loss = 0.0\n",
      "epoch 0: loss = 0.0\n",
      "epoch 0: loss = 0.0\n",
      "epoch 0: loss = 0.0\n",
      "epoch 0: loss = 0.0\n",
      "epoch 0: loss = 0.0\n",
      "epoch 0: loss = 0.0\n",
      "epoch 0: loss = 0.0\n",
      "epoch 0: loss = 0.0\n",
      "epoch 0: loss = 0.0\n",
      "epoch 0: loss = 0.0\n",
      "epoch 0: loss = 0.0\n",
      "epoch 0: loss = 0.0\n",
      "epoch 0: loss = 0.0\n",
      "epoch 0: loss = 0.0\n",
      "epoch 0: loss = 0.0\n",
      "epoch 0: loss = 0.0\n",
      "epoch 0: loss = 0.0\n",
      "epoch 0: loss = 0.0\n",
      "epoch 0: loss = 0.0\n"
     ]
    }
   ],
   "source": [
    "# set up gradient descent params\n",
    "init_lr = 0.1 # default value\n",
    "optimizer = optim.SGD(i3d.parameters(), \n",
    "                      lr=init_lr, \n",
    "                      momentum=0.9, \n",
    "                      weight_decay=0.0000001)\n",
    "lr_sched = optim.lr_scheduler.MultiStepLR(optimizer, [300, 1000])\n",
    "\n",
    "# set up training variables \n",
    "epochs = 200 # random number, change\n",
    "tot_loss = 0.0\n",
    "writer = SummaryWriter(\"deleteme\")\n",
    "step = 0\n",
    "for e in range(epochs):\n",
    "    for data, label in loader:\n",
    "        data = data.cuda()\n",
    "        label = label.cuda()\n",
    "        num_frames = data.size(2)\n",
    "        per_frame_logits = i3d(data).mean(2)\n",
    "        \n",
    "        # compute loss \n",
    "        loss = F.cross_entropy(per_frame_logits, label) \n",
    "        print(f\"epoch {e}: loss = {loss}\")\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step() \n",
    "        writer.add_scalar(\"train/loss\", loss.item(), step) \n",
    "        step+=1\n",
    "    break # remove this to train for more than one epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f014e304-8979-4fa7-b6a0-87432f364518",
   "metadata": {},
   "outputs": [],
   "source": [
    "writer.flush() # ensure that all loss values are recorded "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cos429",
   "language": "python",
   "name": "cos429"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
